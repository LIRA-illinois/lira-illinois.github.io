---
title: "Intrinsic Reward Policy Optimization for Sparse-Reward Environments"
date: 2026-01-29
imageUrl: '/src/assets/research/hierarchical-tree.svg'
imageAlt: "A hierarchical decision policy."
citation: "arXiv'26"
projects: ["hrl"]
paperUrl: '/files/irpo.pdf'
---

<!-- <figure-full-caption>
	<a href="{{ site.url }}{{ site.baseurl }}/assets/images/research/lcss-search.png"><img src="{{ site.url }}{{ site.baseurl }}/assets/images/research/lcss-search.png"></a>
	<figcaption>A partial trace of our search method with weighted Kullback-Leibler (KL) divergence values used to determine which nodes to expand.</figcaption>
</figure-full-caption> -->

<!-- This work proposes a greedy search over a class of **temporal logic formulae** to infer human-interpretable explanations of reinforcement learning policies. See our IEEE Control Systems Letters paper for more details. -->

<!-- <div class="row">
	<a href="https://arxiv.org/pdf/2309.16960" class="button_general">arXiv</a>
	<a href="https://doi.org/10.1109/LCSYS.2024.3519301" class="button_general">DOI</a>
</div> --> -->